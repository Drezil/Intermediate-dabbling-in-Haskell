\documentclass{beamer}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[ngerman]{babel}
\usepackage{graphicx} % Bilder
\usepackage{wrapfig} % Umflussbilder
\usepackage{multicol} % Multiple columns
\usepackage{minted} % Haskell source code
\usepackage{framed} % Frames around source code
\usepackage[framemethod=tikz]{mdframed} % Frames
\usepackage{verbatim} % \begin{comment}...\end{comment}
\usepackage{etoolbox} % manipulate minted
\AtBeginEnvironment{minted}{\fontsize{10}{10}\selectfont}
\AfterEndEnvironment{minted}{}

\mdfdefinestyle{fancy}{
  roundcorner=5pt,
  linewidth=4pt,
  linecolor=red!80,
  backgroundcolor=red!20
}
\newmdenv[style=fancy]{important}

% redifine \em for \emph to use bold instead of italics
\makeatletter
\DeclareRobustCommand{\em}{%
  \@nomath\em \if b\expandafter\@car\f@series\@nil
  \normalfont \else \bfseries \fi}
\makeatother

% Stuff for Beamer
\beamertemplatenavigationsymbolsempty
\usetheme{Warsaw}

\title{Fortgeschrittene Funktionale Programmierung in Haskell}

\begin{document}

%  \usebackgroundtemplate{\includegraphics[width=\paperwidth,height=\paperheight]{1.jpg}} 
  
%----------------------------------------------------------------------------------------  

  \begin{frame}
  \begin{center}
    \huge\textbf{Fortgeschrittene Funktionale Programmierung in Haskell}\\ \bigskip
    \LARGE Universität Bielefeld, Sommersemester 2015\\ \bigskip
    \large Jonas Betzendahl \& Stefan Dresselhaus
    \end{center}
  \end{frame}

%----------------------------------------------------------------------------------------  
\begin{frame}[allowframebreaks]{Outline}
\frametitle{Übersicht}
\tableofcontents[hideallsubsections]
\end{frame}

\begin{frame}
Worum soll es heute gehen?\bigskip

\begin{itemize}
 \item \textbf{Funktionale Programmierung} generell
 \pause
 \item Implementierung einer \textbf{Double-Linked-List}\\
       Wie macht man sowas in Haskell?
 \pause
 \item \textbf{Laziness}\\
       Was für Auswirkungen hat das auf die Programmierung?\\
       Was für Möglichkeiten bietet dies?
\end{itemize}

\end{frame}


\section{Double-Linked List}

\subsection{Anforderungen}

\begin{frame}
Eine Double-Linked-List ist \emph{die} klassische Einstiegsdatenstruktur in der imperativen Welt.\\
\pause\bigskip

Sie besteht aus\dots

\begin{itemize}
 \item einem Paar von (äußeren) Pointern, die auf den Anfang und das Ende zeigen und
 \pause
 \item Elementen, wiederum zusammengesetzt aus:
 \pause
 \begin{itemize}
  \item Einem Pointer auf das nächste Element\\ (\texttt{null} falls Ende der Liste)
  \pause
  \item Einem Pointer auf das vorherige Element\\ (\texttt{null} falls Anfang der Liste)
  \pause
  \item Einem Datum, welches gespeichert werden soll (beliebig)
 \end{itemize}

\end{itemize}

\end{frame}

\begin{frame}
DLL als Grafik:
\begin{figure}[h]
\includegraphics{dll.png}
\end{figure}

\end{frame}

\begin{frame}
Die Vorteile dieser Datenstruktur liegen auf der Hand:
\pause
\begin{itemize}
 \item Einf. eines Elementes vorne/hinten (cons/snoc) liegt in $\mathcal{O}(1)$.
 \pause
 \item Iteration (z.B. \texttt{map}) ist einfach zu implementieren.
 \pause
 \item Finden/Updaten eines Elementes liegt in $\mathcal{O}(n)$.
 \pause
 \item Verbinden von 2 Listen liegt in $\mathcal{O}(1)$.
\end{itemize}
\pause

Die einfache Liste in \texttt{Haskell} hat auch diese Eigenschaften - allerdings nur am vorderen Ende.\pause\bigskip

Einfügen von hinten und Verkettung läuft immernoch in $\mathcal{O}(n)$.\pause\smallskip

Wie bekommen wir nun alle Vorteile nach \texttt{Haskell}-Land? Und wieso gibt es da nichts in der \texttt{Prelude}?

\end{frame}

\subsection{Implementation}

\begin{frame}[fragile]
Also implementieren wir einfach mal diese Datenstruktur:\pause\bigskip

Einen Pointer in \texttt{Haskell} bekommen und bearbeiten wir mittels

\begin{minted}{haskell}
newIORef   :: a -> IO (IORef a)
readIORef  :: IORef a -> IO a
writeIORef :: IORef a -> a -> IO ()
\end{minted}
\pause

Eine Doubly-Linked-List ist dann:
\begin{minted}{haskell}
data Entry a = Entry { prev :: IORef (Maybe (Entry a))
                     , data :: a
                     , next :: IORef (Maybe (Entry a))
                     }
data Dll a   = Dll   { first :: IORef (Maybe (Entry a))
                     , last  :: IORef (Maybe (Entry a))
                     }
\end{minted}
\pause

wobei das \texttt{Maybe}-Konstrukt im \texttt{Entry} einmal den Pointer zurück und einmal den Pointer nach vorn kennzeichnet und das \texttt{Maybe}-Konstrukt im \texttt{Dll} eine leere Liste ermöglicht.
\end{frame}

\begin{frame}[fragile]
Wir können dies nun runter implementieren und erhalten ein Interface ähnlich zu

\begin{minted}{haskell}
newDLL      :: IO (Dll a)
insertFront :: a -> Dll a -> IO (Dll a)
insertBack  :: a -> Dll a -> IO (Dll a)
getFront    :: Dll a -> IO (Maybe a)
getBack     :: Dll a -> IO (Maybe a)
map         :: (a -> b) -> Dll a -> IO (Dll b)
...
\end{minted}
\pause\bigskip

Probleme:
\begin{itemize}
 \item Alles ist in \texttt{IO} (wegen \texttt{IORef}s).
 \pause
 \item Wir können es aufgrund von \texttt{IO} in keinem puren Code benutzen.
 \pause
 \item Wer garantiert uns, dass die Struktur nur Daten hält und (ggf. nach einem \glqq Patch\grqq ) nicht per \texttt{IO} Raketen abschiesst?
\end{itemize}
\pause
Das ist keine gute Lösung! Vor allem nicht funktional!
\end{frame}

\subsection{Fazit}

\begin{frame}
Nochmal Vor- \& Nachteile:\pause\bigskip

Vorteile:
\begin{itemize}
 \item Von den Zugriffszeiten genau das, was wir wollten
\end{itemize}
\pause
Nachteile:
\begin{itemize}
 \item praktisch Unbrauchbar durch \texttt{IO}
 \pause
 \item VIEL Speicheraufwändiger als die C-Lösung, durch
 \pause
 \begin{itemize}
  \item Zusätzliche Pointer im \texttt{Maybe}
  \pause
  \item Zusätzliche Pointer durch das \texttt{IORef}
  \pause
  \item statt drei Pointer (Datum, \texttt{prev} und \texttt{next}) speichern wir sieben (1x \texttt{prev}, 1x \texttt{next}, 2x \texttt{Maybe}, 2x \texttt{IORef}, 1x Datum)
  \pause
  \item \glqq Pointer jagen\grqq \ im Speicher kostet zusätzliche Zeit beim Lookup
 \end{itemize}
\end{itemize}
\pause

Insgesamt ist es keine gute Idee die Datenstrukturen aus der imperativen Programmierung einfach nachzubauen. \\\par\pause
Aber wie geht es dann?
\end{frame}

\section{Haskell-Lösungen}
 
\subsection{Anforderungen}
\begin{frame}
Wir fangen einfach mal an mit einem Wunschkonzert. Wir hätten gerne:
\pause
\begin{itemize}
 \item Ein Sequence-Ähnliches Ding (Array, Liste, etc.)
 \pause
 \item Schnelles Hinzufügen/Entfernen von Elementen am Anfang/Ende
 \pause
 \item Schnelles Zusammenfügen zweier dieser Dinge
 \pause
 \item Iterieren (i.e. \texttt{map}) über dieses Ding
 \pause
 \item Möglichkeit, Elemente aus der Mitte zu entfernen
 \pause
\end{itemize}
\smallskip

und weil wir schonmal dabei sind:
\begin{itemize}
 \item immutable und pure
 \pause
 \item Wenig Speicherverbrauch
 \pause
\end{itemize}
Die erste Liste ist die typische Problemstellung deren Lösung eine Doubly-Linked-List ist - mit den Zusatzanforderungen müssen wir uns was anderes überlegen.
\end{frame}

\begin{frame}[fragile]
Wie könnte so eine API (minimal) aussehen?
\pause\bigskip

\begin{minted}{haskell}
data Ding a = ... -- implementation goes here

empty    :: Ding a
isEmpty  :: Ding a -> Bool
append   :: Ding a -> a -> Ding a
prepend  :: Ding a -> a -> Ding a
getFirst :: Ding a -> Maybe (a, Ding a)
getLast  :: Ding a -> Maybe (a, Ding a)
concat   :: Ding a -> Ding a -> Ding a
\end{minted}
\end{frame}

\begin{frame}[fragile]
Wenn das alles ist, dann können wir das auch zu einer Typklasse machen
\pause\bigskip

\begin{minted}{haskell}
class Deque d where
  empty    :: d a
  isEmpty  :: d a -> Bool
  append   :: d a -> a -> d a
  prepend  :: d a -> a -> d a
  getFirst :: d a -> Maybe (a, d a)
  getLast  :: d a -> Maybe (a, d a)
  concat   :: d a -> d a -> d a
\end{minted}
\end{frame}

\subsection{Zwei Listen}

\begin{frame}[fragile]
Mögliche Lösung: Zwei Listen\bigskip

\begin{minted}{haskell}
data TwoLists a = TwoLists { front :: [a]
                           , back  :: [a]
                           }
\end{minted}
\pause\bigskip

Wie implementieren wir jetzt die API?
\end{frame}

\begin{frame}[fragile]
API (cont.)

\begin{minted}{haskell}
empty = TwoLists [] []
\end{minted}
\pause
\begin{minted}{haskell}
isEmpty (TwoLists [] []) = True
isEmpty _                = False
\end{minted}
\pause
\begin{minted}{haskell}
prepend a (TwoLists front back) = TwoLists (a:front) back
\end{minted}
\pause
\begin{minted}{haskell}
append  a (TwoLists front back) = TwoLists front (a:back)
\end{minted}
\end{frame}

\begin{frame}[fragile]
\begin{minted}{haskell}
getFirst   (TwoLists []     []) = Nothing
getFirst   (TwoLists (a:as) bs) = Just (a, TwoLists as bs)
getFirst   (TwoLists []     bs  = let (c:cs) = reverse bs in
                                  Just (c, TwoLists cs [])
\end{minted}
\pause
\begin{minted}{haskell}
getLast    (TwoLists []     []) = Nothing
getLast    (TwoLists as (b:bs)) = Just (b, TwoLists as bs)
getLast    (TwoLists as     []) = let (c:cs) = reverse as in
                                  Just (c, TwoLists [] cs)
\end{minted}
\pause
\begin{minted}{haskell}
concat (TwoLists as bs) (TwoLists cs ds) =
        TwoLists (as ++ reverse bs) (ds ++ reverse cs)
\end{minted}
\end{frame}

\subsection{Fazit: 2 Listen}


\begin{frame}
Diese Struktur erfüllt alle Dinge, die wir aus dem Interface haben wollten, aber...
\pause
\begin{itemize}
 \item wir benutzen \texttt{reverse}, welchen Laufzeit $\mathcal{O}(n)$ hat.\\
 \pause
       Dies zieht alle Operationen, die dies benutzen auf $\mathcal{O}(n)$ hoch.\\
\end{itemize}
\pause\bigskip

Asymptoten:
\begin{multicols}{2}
\begin{itemize}
 \item empty $\mathcal{O}(1)$
 \item append $\mathcal{O}(1)$
 \item prepend $\mathcal{O}(1)$
\end{itemize}
\columnbreak
\begin{itemize}
 \item getFirst $\mathcal{O}(n)$
 \item getLast $\mathcal{O}(n)$
 \item iterate $\mathcal{O}(n^2)$
\end{itemize}
\end{multicols}

\pause
Die $\mathcal{O}(n)$-Operation kommt zwar nur selten vor, aber wenn wir z.B. abwechselnd \texttt{getFirst} und \texttt{getLast} machen, machen wir diese teure Operation jedes mal.
\end{frame}

\subsection{Finger-Trees}

\begin{frame}
Gibt es da nichts besseres?\pause\smallskip

Ja, aber das ist nicht ganz so simpel.\pause\bigskip

Wir müssen dazu einen Exkurs in Bäume machen. Wir starten mit einem klassischen 2-3-Tree, also einem Baum mit entweder zwei oder drei Kind-Knoten pro Elternknoten.
\end{frame}

\begin{frame}
\begin{figure}
\includegraphics[width=\textwidth]{finger-tree-0.png}
\end{figure}
\end{frame}

\begin{frame}
\begin{figure}
\includegraphics[width=\textwidth]{finger-tree-1.png}
\end{figure}
\end{frame}

\begin{frame}
\begin{figure}
\includegraphics[width=\textwidth]{finger-tree-2.png}
\end{figure}
\end{frame}

\begin{frame}
\begin{figure}
\includegraphics[width=\textwidth]{finger-tree-3.png}
\end{figure}
\end{frame}

\begin{frame}
So ein Baum hat in jeder Ebene ein Präfix, den Rest des Baumes und ein Suffix.\\\par\pause
In der ersten Ebene hat er zwei oder drei Kind-Knoten\\
In der zweiten Ebene einen oder zwei Kind-Bäume der Tiefe 1\\
In der dritten Ebene einen oder zwei Kind-Bäume der Tiefe 2 etc.\pause\bigskip

Wir nennen Präfixe und Suffixe nun \emph{Affixe} und sagen, dass diese bis zu vier Elemente haben können. Dies bringt uns später Vorteile bei der Laufzeit.
\end{frame}

\begin{frame}[fragile]
Kurz in Code, was wir bisher definiert haben:
\begin{minted}{haskell}
data Node  a = Branch2 a a
             | Branch3 a a a
             deriving Show
\end{minted}
\pause
\begin{minted}{haskell}
data Affix a = One   a
             | Two   a a
             | Three a a a
             | Four  a a a a
             deriving Show
\end{minted}
\pause
\begin{minted}{haskell}
data FingerTree a 
  = Empty      -- We can have empty trees.
  | Single a   -- We need a special case for trees of size one.
  -- The common case with a prefix, suffix, and a deeper tree.
  | Deep {
    prefix :: Affix a,             -- Values on the left.
    deeper :: FingerTree (Node a), -- The deeper finger tree,
                                   -- storing deeper 2-3 trees.
    suffix :: Affix a              -- Values on the right.
  }
  deriving Show
\end{minted}
\end{frame}

\begin{frame}
Was haben wir hiermit?\\\par\pause
\begin{itemize}
 \item Einen Baum, der uns die \glqq Enden\grqq \ direkt präsentiert
 \pause
 \item Jede Ebene \texttt{Deep} fügt eine weitere \texttt{Node} hinzu\pause\smallskip
 
       \texttt{FingerTree (Node a)} hat einen Affix von $2\cdot 1$ (\texttt{One (Branch2 a)}) bis $3\cdot 4$ (\texttt{(Four (Branch3 a)}) Elementen\pause\smallskip
       
       \texttt{FingerTree (Node (Node a))} hat einen Affix von $2 \cdot 2\cdot 1$ bis $3 \cdot 3\cdot 4$ Elementen\\\pause
       etc.
 \pause
 \item Wir haben auf jeder Ebene bis zu 3x mehr Elemente als in den vorhergehenden.
 \pause
 \item Der Baum ist automatisch balanciert (nicht perfekt), da jede Ebene sowohl Präfix als auch Suffix-Elemente haben muss
\end{itemize}
\end{frame}

\begin{frame}[fragile]
Wie fügen wir nun Elemente ein?\pause
\begin{minted}{haskell}
infixr 5 <|
(<|) :: a -> FingerTree a -> FingerTree a

x <| Empty = Single x
x <| Single y = Deep (One x) Empty (One y)
x <| Deep (Four a b c d) deeper suffix =
  Deep (Two x a) (node <| deeper) suffix
  where
    node = Branch3 b c d
x <| tree = tree { prefix = affixPrepend x $ prefix tree }
\end{minted}
gegeben eine Funktion \texttt{affixPrepend}, die aus einem \texttt{One} ein \texttt{Two} macht etc.
\end{frame}

\begin{frame}[fragile]
Analog hinten
\begin{minted}{haskell}
infixr 5 |>
(|>) :: FingerTree a -> a -> FingerTree a

Empty |> x = Single x
Single y |> x = Deep (One y) Empty (One x)
Deep prefix deeper (Four a b c d) |> x =
  Deep prefix (deeper |> node) (Two d x)
  where
    node = Branch3 a b c
tree |> x = tree { suffix = affixAppend x $ suffix tree }
\end{minted}
gegeben eine Funktion \texttt{affixAppend}, die aus einem \texttt{One} ein \texttt{Two} macht etc.
\end{frame}

\begin{frame}
Aber wie schaut denn nun die Laufzeit aus? Dieses Einfügen führt doch im schlimmsten Falle dazu, dass alle Ebenen angefasst und neu aufgebaut werden müssen!\\\pause
Richtig. Aber wie häufig?\\\par\pause
Jede Operation macht eine $\mathcal{O}(1)$-Operation in der obersten Ebene. Mit Wahrscheinlichkeit $\frac{1}{2}$ machen wir auch in Ebene 2 eine $\mathcal{O}(1)$-Operation.
\end{frame}

\begin{frame}
Generell machen wir für $m$ Einfüge-Operationen (von derselben Seite)
$$T = m + \frac{1}{2}m + \frac{1}{4}m + ... = \sum_{i=0}^m \frac{1}{2^i} m$$
\pause
Operationen, was uns allen bekannt vorkommen sollte. Für $m \to \infty$ Einfügeoperationen brauchen wir folglich
$$m \cdot \lim_{m \to \infty} \sum_{i=0}^m \frac{1}{2^i} = m \cdot 2$$
Operationen, welches uns armortisiert pro Operation $\mathcal{O}(1)$ kostet.
\end{frame}

\begin{frame}
Das letzte bzw. erste Element bekommen ist ähnlich kompliziert und mit derselben Argumentation kommt man auch hier auf eine Laufzeit von $\mathcal{O}(1)$ für beide Operationen.\pause\bigskip

Wir werden hier nur kurz den Code angugcken, damit er auf den Folien ist. Weitere Informationen findet man in dem (sehr guten!) Blogpost von Andrew Gibiansky unter \url{http://andrew.gibiansky.com/blog/haskell/finger-trees/}
\end{frame}

\begin{frame}[fragile]
Zunächst definieren wir uns eine View-Datenstruktur. Diese ist theoretisch isomorph zu \texttt{Maybe}, macht aber im Kontext der Verwendung Sinn. \bigskip

\begin{minted}{haskell}
data View a = Nil | View a (FingerTree a)
  deriving Show
\end{minted}
\end{frame}

\begin{frame}[fragile]
\begin{minted}{haskell}
viewl :: FingerTree a -> View a
viewl Empty = Nil              
viewl (Single x) = View x Empty
viewl (Deep (One x) deeper suffix) = View x rest
  where
    rest =
      case viewl deeper of
        View node rest' ->
          Deep (fromList $ toList node) rest' suffix
        Nil -> case suffix of
          (One x) -> Single x
          (Two x y) -> Deep (One x) Empty (One y)
          (Three x y z) -> Deep (Two x y) Empty (One z)
          (Four x y z w) -> Deep (Three x y z) Empty (One w)
viewl (Deep prefix deeper suffix) =
  View first $ Deep (fromList rest) deeper suffix
  where
    first:rest = toList prefix
\end{minted}
mit \texttt{fromList} und \texttt{toList} definiert auf \texttt{Affix}
\end{frame}

\begin{frame}
Der Code für \texttt{viewr} ist analog hierzu.\\\par\pause
Wir haben noch das verketten von zwei Finger-Trees:\\
Da wir in $\mathcal{O}(1)$ \texttt{view} und \texttt{append} machen können, können wir in $\mathcal{O}(m)$ Zeit einen Baum der Länge $m$ an einen Baum der Länge $n$ anhängen.\pause
\bigskip

Dies geht aber auch geschickter über die Struktur des Trees. Für den Code sei auf die Literatur verwiesen. Hier nur die Laufzeit: $\mathcal{O}(\log (min(n,m)))$.
\end{frame}

\begin{frame}[fragile]
Mit dieser Struktur geht sogar noch (viel!) mehr.\\\par\pause
Gegeben einen Monoid $m$ können wir Annotationen von Typ $m$ hinzufügen.\\\pause
Wenn wir dann weiterhin so etwas wie
\begin{minted}{haskell}
class Monoid v => Measured a v where
  measure :: a -> v
\end{minted}
vorraussetzen, dann ergibt sich insgesamt
\end{frame}

\begin{frame}[fragile]
\begin{minted}{haskell}
data Node v a = Branch3 v a a a
              | Branch2 v a a
              deriving Show
            
data FingerTree v a 
  = Empty
  | Single a
  | Deep {
    annotation :: v, -- Add an annotation to each branch.
    prefix :: Affix a,
    deeper :: FingerTree v (Node v a),
    suffix :: Affix a
  }
  deriving Show
\end{minted}
für die Definition des FingerTrees.\\\pause
Alle bisherigen Operationen müssen natürlich angepasst werden, um die Annotationen mit durchzuschleifen. An der Laufzeit ändert sich nichts.
\end{frame}

\begin{frame}[fragile]
Einen FingerTree kann man dann messen durch
\begin{minted}{haskell}
instance Measured a v => Measured (FingerTree v a) v where
  measure Empty = empty
  measure (Single x) = measure x
  measure tree = annotation tree

instance Measured a v => Measured (Node v a) v where
  measure (Branch2 v _ _) = v
  measure (Branch3 v _ _ _) = v
\end{minted}
\end{frame}

\begin{frame}[fragile]
Frage: Was bringt uns das Ganze?\\\pause
Mittels
\begin{minted}{haskell}
-- Monoidal size - all leaves have Size 1.
newtype Size = Size Int deriving (Show, Eq, Ord)

-- Storage for our values.
newtype Value a = Value a deriving Show

-- Sizes just add normally.
instance Monoid Size where
  mempty = Size 0
  Size x <> Size y = Size $ x + y
  
-- All values just have size one.
instance Measured (Value a) Size where
  measure _ = Size 1
\end{minted}
%$
Nummerieren wir die Elemente durch. Somit können wir in $\mathcal{O}(log(n))$ auf das $n$-te Element zugreifen!
\end{frame}

\begin{frame}[fragile]
Frage: Was bringt uns das Ganze (2)?\\\pause
Mittels
\begin{minted}{haskell}
data Prioritized a = Prioritized {
    priority :: Int,
    item :: a }
data Priority = NegativeInfinity | Priority Int deriving Eq
instance Monoid Priority where
  NegativeInfinity <> x = x
  x <> NegativeInfinity = x
  (Priority x) <> (Priority y) = Priority $ max x y
  empty = NegativeInfinity
instance Measured (Prioritized a) Priority where
  measure = Priority . priority
newtype PriorityQueue a = PriorityQueue (FingerTree Priority
                                              (Prioritized a))
\end{minted}
%$
wandeln wir den FingerTree in eine Priority-Queue, wo wir schnell auf das Element mit der höchsten Priorität zugreifen können.
\end{frame}

\begin{frame}[fragile]
Frage: Was bringt uns das Ganze (3)?\pause\bigskip

Da wir Objekte schnell lokalisieren können (in $\mathcal{O}(\log(n))$), können wir auch einen schnellen Split an so einer Stelle machen.
\end{frame}

\subsection{Fazit: Finger-Trees}

\begin{frame}[fragile]
Was haben wir gelernt?\\\pause
Die simpelste Lösung ist nicht immer die Beste, aber wir haben ausgehend von doppelt verketteten Listen eine gute Datenstruktur gefunden.\\\pause
Abschließend ein kurzer Überblick über Laufzeiten gängiger Strukturen.
\tiny
\begin{table}
\begin{tabular}{l|llll}
\multicolumn{1}{c}{Operation} & \multicolumn{4}{c}{Amortized Bounds}\\\hline
 & Finger Tree & Annotated 2-3 Tree & List & Vector\\

cons/snoc & $\mathcal{O}(1)$ & $\mathcal{O}(\log n)$ & $\mathcal{O}(1)$/$\mathcal{O}(n)$ & $\mathcal{O}(n)$ \\
viewl/viewr & $\mathcal{O}(1)$ & $\mathcal{O}(\log n)$ & $\mathcal{O}(1)$/$\mathcal{O}(n)$ & $\mathcal{O}(1)$\\
measure/lenth & $\mathcal{O}(1)$ & $\mathcal{O}(1)$ & $\mathcal{O}(n)$ & $\mathcal{O}(1)$\\
append & $\mathcal{O}(\log \min(l_1,l_2))$ & $\mathcal{O}(\log n)$ & $\mathcal{O}(n)$ & $\mathcal{O}(n + m)$ \\
split & $\mathcal{O}(\log \min (n,l-n))$ & $\mathcal{O}(\log n)$ & $\mathcal{O}(n)$ & $\mathcal{O}(1)$ \\
fromList/toList/reverse & $\mathcal{O}(l)$/$\mathcal{O}(l)$/$\mathcal{O}(l)$ & $\mathcal{O}(l)$ & $\mathcal{O}(1)$/$\mathcal{O}(1)$/$\mathcal{O}(n)$ & $\mathcal{O}(n)$ \\
index & $\mathcal{O}(\log \min(n,l-n))$ & $\mathcal{O}(\log n)$ & $\mathcal{O}(n)$ & $\mathcal{O}(1)$
\end{tabular}
\end{table}
\normalsize
\end{frame}

\begin{frame}[fragile]
FingerTrees sind die Datenstruktur, wenn man viele Anfüge und Entfern-Operationen hat. Daher basiert auch alles in \texttt{Data.Sequence} intern auf FingerTrees.\\\pause
Auch sind sie Doppelt-Verketteten Listen in allen praktischen Anwendungsbereichen überlegen, da sie auch effizienten Random-Access erlauben ($\mathcal{O}(\log n)$ statt $\mathcal{O}(n)$) und sich in $\mathcal{O}(\log n)$ zerteilen lassen (statt $\mathcal{O}(n)$).\\\pause\par\bigskip
Es gibt noch weitergehende Strukturen, die hierauf aubauen. Edward Kmett hat z.B. \texttt{Ropes} vorgestellt, welche um einen skalaren Faktor besser und dabei Speicher- und Cachefreundlicher (0.7-30x weniger Speicheraufwand, weniger GC-Zeit) sind und unboxed Strukturen erlauben.
\end{frame}

\section{Laziness}

\subsection{Allgemein}

\begin{frame}
\begin{center}
\textbf{Laziness}\bigskip

\includegraphics[scale=0.15]{maxresdefault.jpg} 
\end{center}
\end{frame}

\begin{frame}[fragile]
Was macht Laziness so besonders?\pause
\begin{itemize}
 \item Trennug von Definition und Evaluation\\\pause
       Wir können Dinge seperat definieren, aber gemeinsam berechnen
 \pause
 \item Vermeidung von unnötigen Berechnungen\\\pause
\begin{minted}{haskell}
select :: Ord o => Int -> [o] -> [o]
select n = take n . sort
\end{minted}
       \pause
       hört mit dem sortieren auf, sobald die ersten $n$ kleinsten Elemente gefunden wurde.\\\pause
       Wie das hinzufügen eines \texttt{break} in imperativem Code (innerhalb der Sortierfunktion)
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\begin{itemize}
 \item Lazy Datenstrukturen sind im Prinzip eine Steuerung des Kontrollflusses.\\\pause
       z.B. entspricht eine Liste einem For-Loop in imperativen Programmiersprachen
 \pause
 \item Lazy Datenstrukturen sind aber First-Class-Citizens in Haskell, sprich\\\pause
       \begin{itemize}
        \item wir können sie an Funktionen übergeben
        \pause
        \item auf sie Pattern-Matchen
        \pause
        \item sie zu größeren Dingen komponieren
       \end{itemize}
 \pause
 \item Lazy Datenstrukturen brauchen nicht komplett im Speicher liegen, sondern werden erstellt, wenn sie benötigt werden\\\pause
       \begin{itemize}
        \item werden sofort GC, wenn benutzt
        \pause
        \item \begin{minted}{haskell}
factorial n = product [1..n]
              \end{minted}
              wird zu einem for-loop mit constant-memory usage
        \pause
        \item \mint{haskell}|fold . f . unfold|
              ist klassisch in der Datenverarbeitung

       \end{itemize}

\end{itemize}

\end{frame}

\subsection{Non-Deterministic Programming}

\begin{frame}[fragile]
Wenn eine Liste ein Loop ist, dann sind verschachtelte Listen verschachtelte Loops. Dies ermöglicht uns non-deterministische Programmierung.\pause\smallskip\smallskip

Wir kennen das schon von der Monaden-Instanz von List:
\begin{minted}{haskell}
do a <- [1..10]
   b <- [1..10]
   guard (a /= b && a + b = 7)
   return (a,b)
\end{minted}

Es existieren nie alle 100 Kombinationen von \texttt{a} und \texttt{b} im RAM, sondern nur diejenigen, die das Prädikat überleben und die nächste, die probiert wird.
\end{frame}

\begin{frame}
\textbf{Kartenfärbungsproblem:}\bigskip

Gegeben eine Karte mit Staaten, wollen wir nun eine Einfärbung der Karte errechnen - wobei keine benachbarten Staaten dieselbe Farbe bekommen sollen.

\begin{center}
\includegraphics[scale=0.1]{europe-map-colour-printable.jpg} 
\end{center}
\end{frame}

\begin{frame}[fragile]
Wenn wir die Funktion

\begin{minted}{haskell}
step :: Map -> Country -> [Map]
\end{minted}

haben, die uns einen beliebigen Staat entsprechend den Kriterien einfärbt, dann ist\pause

\begin{minted}{haskell}
solutions :: [Map]
solutions = foldM step blank states
\end{minted}

die Liste, die alle validen Einfärbungen enthält.
\end{frame}

\begin{frame}[fragile]
Mittels einem Ausdruck wie
\begin{minted}{haskell}
first = head solutions
\end{minted}
bekommen wir irgendeine Lösung.\\\pause
Aber wir können auch Anforderungen an die Lösung haben:
\begin{minted}{haskell}
some = find gerBlue solutions
all = filter getBlue solutions
\end{minted}
\pause
Dies gibt uns eine (oder alle) Karten, bei denen Deutschland blau gefärbt ist.\\\pause\par\bigskip
Durch Laziness haben wir nie alle Lösungen im Speicher sondern immer nur die minimale Anzahl.
\end{frame}

\subsection{Precision on Demand}

\begin{frame}[fragile]
Laziness kann auch Präzision in Datenstrukturen nachliefern, weil wir eine unendlich genaue Datenstruktur definieren können.\pause

Es gibt ein Paket (\url{https://github.com/mvr/cf}), welches mit sogenannten continued fractions rechnet.\pause
Eine genaue Erklärung gibt es unter \url{https://www.youtube.com/watch?v=LJQgYBQFtSE}. Kurz zusammengefasst:\\\pause

\begin{itemize}
 \item Jede rationale Zahl lässt sich als endliche Liste darstellen
 \pause
 \item $\pi,e$ lassen sich durch unendlich lange Listen darstellen (lazy)
 \pause
 \item alle Operationen $+ - \times /$ konsumieren soviel von der Liste, wie sie brauchen
 \pause
 \item am ENDE legt man die Genauigkeit fest, die man gerne hätte und alle Zahlen bis da hin werden nur so weit berechnet, wie man für die gegebene Genauigkeit braucht.
\end{itemize}

\end{frame}

\subsection{Memoization}

\begin{frame}
Laziness ist ein \glqq built-in controlled side-effect\grqq , welcher unter unserer Programmierschwelle liegt. Der Compiler kümmert sich also darum, dass\pause
\begin{itemize}
 \item Wir lazy Werte höchstens 1x berechnen und ggf. cachen, wenn wir sie später nochmal brauchen
 \item alles deterministisch läuft (obwohl wir nicht-deterministisch schreiben können)
 \item alles thread-safe abläuft (und wir somit einfach parallelisieren können)
\end{itemize}
\pause
All das bekommen wir durch Laziness kostenlos über das Runtime-System und wir können hierbei keine Fehler mehr machen (sofern wir uns von unsafe-Functions fernhalten).
\end{frame}

\begin{frame}[fragile]
Beispiel:
\begin{minted}{haskell}
fibs :: [Integer]
fibs = 0 : 1 : zipWith (+) fibs (tail fibs)
\end{minted}
\pause
\bigskip

Wenn wir nun die $n$-te Fibonnaci-Zahl haben wollen, dann
\begin{itemize}
 \item errechnen wir die nächste Zahl aus den vorhergehenden
 \pause
 \item sammelt der GC die Zahlen wieder ein, die wir nicht brauchen
 \pause
 \item haben wir maximal zwei Integer und die Berechnung für die nächste Zahl im Speicher
\end{itemize}
\pause
Außerdem können wir diese (undendliche) Liste auch mit allen anderen Funktionen manipulieren.
\end{frame}

\begin{frame}[fragile]
Generell nennt man das Zwischenspeichern von Teilergebnissen \textbf{Memoization}.\\\par\pause
Dieses kann entweder automatisch geschehen oder muss manuell gemacht werden.\\\par\pause
In Haskell können wir uns nicht immer auf den Compiler verlassen das automatisch zu tun, aber es gibt Pakete, die das garantieren:
\begin{itemize}
 \item \texttt{data-memocombinators}
 \item \texttt{MemoTrie}
 \item infinite lazy trees
\end{itemize}
\end{frame}

\subsection{Dynamic Programming}

\begin{frame}[fragile]
Ein weiterer Fall für die Anwendung von Laziness gibt es in der \textbf{dynamischen Programmierung}.\\\par\pause
Hier wird eine Beschleunigung des Programmes dadurch erreich, dass man gezielt Zwischenspeicher (klassischerweise Arrays) anlegt und immer wieder auf diese zugreift.\\\par\pause
So kann man mit Hilfe von \glqq Bellmans Principle\grqq \ (auf das wir nicht weiter eingehen) einen exponentiellen Suchraum in polynomieller Zeit auf ein Ergebnis hin untersuchen.
\end{frame}

\begin{frame}[fragile]
Häufig wollen wir in der dynamischen Programmierung also einen Lookup-Array haben.\\\pause
Wir können hier einfach einen Array mit der Berechnung lazy erzeugen und das RTS kümmert sich darum, dass jeder Wert zur richtigen Zeit gefüllt ist und keiner doppelt berechnet wird.\\\pause
Beispiel:
\begin{minted}{haskell}
fib :: Integer -> Integer
fib 0 = 0
fib 1 = 1
fib n = fibs ! (n-1) + fibs ! (n-2)
  where
    fibs = Array (0,n) [fib i | i <- [0..n]]
\end{minted}
\pause
\begin{itemize}
 \item Für Fibonnaci overkill, da wir immer nur zwei Elemente \glqq nach hinten\grqq\ schauen müssen
 \pause
 \item Für Aufgaben, die beliebig \glqq nach hinten\grqq\ sehen müssen (Strig-Edit-Distance, etc.) ist dies aber die beste Lösung.
\end{itemize}

\end{frame}

\section{Fazit: Laziness}

\begin{frame}[fragile]
Was haben wir nun von Laziness?\pause
\begin{itemize}
 \item Modularität\\\pause
       Wir können Definition und Evaluation trennen
 \pause
 \item Kontrollfluss\\\pause
       Wir können über lazy Datenstrukturen den Kontrollfluss ändern und später definieren, was wir eigentlich haben wollen
 \pause
 \item Präzision\\\pause
       Wir können beliebig präzise rechnen (arbitrary precision floating point, zahlreiche 2D-Anwendungen ähnlich zu Vektorgrafiken)
 \pause
 \item Memoization\\\pause
       Der Compiler optimiert unseren Code, indem er automatisch sachen cached, welche mehrfach verwendet werden
\end{itemize}
\pause
Mehr Informationen/Quellen:\\
\url{http://begriffs.com/posts/2015-06-17-thinking-with-laziness.html}
\end{frame}


\end{document}